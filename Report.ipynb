{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continous Control Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Reinforcement Learning Continous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Overview\n",
    "\n",
    "In this project, 20 identical Agents were built using Deep Deterministic Policy Gradent (DDPG) algorithm to solve the Reacher Environment.Each agent has its own copy of the environment.\n",
    "\n",
    "#### Environment\n",
    "For this project, I worked with the Reacher environment.\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "I solved the 2nd version of the Unity environment: \n",
    "The 2nd version contains 20 identical agents, each with its own copy of the environment.\n",
    "\n",
    "In order to solve the environment, the agents must achieve an average score of +30 (over 100 consecutive episodes, and over all agents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology and Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "In this project, a single brain was used to control all 20 agents, rather than 20 individual brain for each agent. A policy based method was adopted beacause of adaptability to continuous action spaces and the fact that policy based methods can learn the optimal policy directly, without maintaining a separate value function estimate.\n",
    "\n",
    "An extension of Deep Q-learning to continuos tasks called Deep Deterministic Policy Gradient (DDPG) algorithm was implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DDPG_Algorithm.png](./images/DDPG_Algorithm.png)\n",
    "\n",
    "The researchers at Google Deepmind in this paper [ontinuous Control with Deep Reinforcement Learning](https://arxiv.org/pdf/1509.02971.pdf) presented a model-free, off-policy actor critic algorithm using deep function approximators. This function is capable of learning policies in high dimentional continous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Structure\n",
    "There are 3 main files that are very important for the implementation of this project. The files were structured this way for the sake of Modularity and easy debugging.\n",
    "\n",
    "```model.py```: In this file I implemented the model for Actor and the Critic class using PyTorch framework. This Method - Actor-Critic method combines both Value-based and Policy-based methods.\n",
    "\n",
    "- ```class Actor```\n",
    "\n",
    "    - An Input Layer: Which the value depends on the state_size parameter.\n",
    "    - A BatchNorm1D layer: This was added immediately after the first layer to scale the features and ensure that they are in the same range throughout the entire model.\n",
    "    - one other fully connected layer with in_units=400 and out_units=300.\n",
    "    - An output layer: The value of which depends on the action_size parameter.\n",
    "    - ```.reset_parameters()```: This methods helps to initialize the weights using uniform distribution.\n",
    "    - ```.forward()```: method maps states to corresponding actions. A non-linear function called ReLu activation function was used for the hidden layers and tanh was used for the output layer to maintain values between -1 and 1.\n",
    "\n",
    "**Actor Architecture**\n",
    "\n",
    "     Input nodes(33)\n",
    "    (BatchNorm1D) BatchNorm Layer (400 nodes, ReLU activation)\n",
    "    (fc) fully connected linear layer (300 nodes, ReLU activation)\n",
    "    Output nodes(4, tanh activation) \n",
    "    \n",
    "    \n",
    "    \n",
    "- ```class Critic```\n",
    "    - Input Layer: The size depends on the state_size parameter.\n",
    "    - Two (2) layers(BatchNorm1D and a fully connected linear layer): the reason for using the batchnorm layer is still the same as in the Actor Class. The fully connected linear layer has in_units which is equal to the 400+action_size and out_unis=300.\n",
    "    - Ouput layer: this layer gives a single value.\n",
    "    - ```.reset_parameters()```: This methods helps to initialize the weights using uniform distribution.\n",
    "    - ```.forward()```: this method implements the forward pass and maps (state action) pair. ReLu activation function was used for the hidden layers. The output of the first activation layer was concatenated with action value. No activation function was for the output layer.\n",
    "    \n",
    "**Critic Architecture**\n",
    "\n",
    "     Input nodes(33)\n",
    "    (BatchNorm1D) BatchNorm Layer (400 nodes, ReLU activation)\n",
    "    (fc) fully connected linear layer (300+action nodes, ReLU activation)\n",
    "    Output nodes(1)    \n",
    "      \n",
    "<br>  \n",
    "    \n",
    "```Agent.py```: This file contains the implementation of the Action-Critic logic, Ornstein-Uhlenbeck Process and Experience Replay.\n",
    "\n",
    "- ```Class Agent```:\n",
    "    - The local and target networks were initialized separately for both the action and the critic to improve stability. I also instatiate OUNoise and ReplayBuffer.\n",
    "    - ```.step()```: this method implement the interval in which the learning step is only performed every 20 timesteps (LEARN_EVERY = 20). It saves and samples experiences from the Replay Buffer and run .learn() for range(LEARN_NUM = 10).\n",
    "    - ```.act()```: The method return Actions for a given state based on the current policy. In this method the noise parameter is accompanied by an epsilon parameter used to decay the level of noise.\n",
    "    - ```.learn()```: Here, the policy value parameters were updated with selected experiences. The critic network was first implemented, after the forward pass, I calculated the loss and before the optimiation step, the gradient was clipped to deal with exploding gradient problem. Later the Actor network was implemented with clipping its gradient and the noise was also updated using EPSILON_DECAY\n",
    "    - ```.soft_update()```: The model parameters were updated here.\n",
    "   \n",
    "   \n",
    "- ```Class OUNoise```: In this method [Ornstein-Uhlenbeck Process](https://arxiv.org/pdf/1509.02971.pdf) was implemented. This process adds a certain amount of noise to the action values at each timestep and help us address the trade-off between Exploitation Vs. Exploration Dilema. This was originally implemented in CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING paper.\n",
    "\n",
    "    - parameters (mu, theta, sigma, seed) were initialized.\n",
    "    - ```.reset()```: It create a copy of the internal state  with parameter, mu.\n",
    "    - ```.sample()```: This update the internal state and return it as a noise sample using theta and sigma parameters.\n",
    "\n",
    "\n",
    "- ```Class ReplayBuffer```: In this class, experience replay was implemented, which allows the Agent to learn from past experiences. So this fixed size buffer can store experince tuples. For the 20 agents, we have just one central replay buffer to enable the agents learn from each others' experiences since they are performing the same task. Experience is selected by each agent stochastically.\n",
    "\n",
    "    - The replay buffer parameters and experience tuple were initialized.\n",
    "    - ```.add()```: The method adds new Experience tuple _(state, action, reward, next_state, done)_ to the memory\n",
    "    - ```.sample()```: This samples and return Random batch of experiences from the memory.\n",
    "    \n",
    "<br>\n",
    "\n",
    "```Continous_Control.ipynb```: This notebook consist of codes for training the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "These are hyperparameters used in the Agent.py file.\n",
    "\n",
    "```\n",
    "BUFFER_SIZE       = int(1e6)  \n",
    "BATCH_SIZE        = 128        \n",
    "GAMMA             = 0.99            \n",
    "TAU               = 1e-3              \n",
    "LR                = 1e-3         \n",
    "WEIGHT_DECAY      = 0        \n",
    "LEARN_EVERY       = 20        \n",
    "LEARN_NUM         = 10          \n",
    "OU_SIGMA          = 0.2          \n",
    "OU_THETA          = 0.15         \n",
    "EPSILON           = 1.0           \n",
    "EPSILON_DECAY     = 1e-6\n",
    "```\n",
    "\n",
    "For the Actor and Critic Network ```Adam Optimizer``` was used with learning rate (LR) of 1e-3 each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "After training the Agents with the specified hyperparamters and architecture, the plot below was generated. The plot shows the performance of the agents over several episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result.png](./images/result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graph.png](./images/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas on Performance Improvement\n",
    "In the future, I will consider improvement on this project using:\n",
    "\n",
    "- **Priotized Experience Replay**: Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. [Paper](https://arxiv.org/abs/1511.05952)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
